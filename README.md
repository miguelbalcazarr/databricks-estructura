
# ğŸ§± Proyecto Base: Datalakehouse en Azure Databricks

Este repositorio define una arquitectura base para implementar proyectos Datalakehouse sobre Azure Databricks, con enfoque modular por dominio, separaciÃ³n de ambientes, y buenas prÃ¡cticas de desarrollo y orquestaciÃ³n.

---

## ğŸ“¦ **Estructura del Proyecto**

```text
databricks-estructura-main/
â”œâ”€â”€ .azure-pipelines/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ ingestion/
â”‚       â””â”€â”€ dominio_1/
â”‚           â”œâ”€â”€ arquetipos/
â”‚           â”‚   â””â”€â”€ ntb_arquetipo.ipynb
â”‚           â”œâ”€â”€ bronze/
â”‚           â”‚   â”œâ”€â”€ ntb_bronze.ipynb
â”‚           â”‚   â””â”€â”€ ntb_create_tables.ipynb
â”‚           â”œâ”€â”€ gold/
â”‚           â”‚   â””â”€â”€ ntb_gold.ipynb
â”‚           â”œâ”€â”€ silver/
â”‚           â”‚   â””â”€â”€ ntb_silver.ipynb
â”‚           â”œâ”€â”€ tables/
â”‚           â”‚   â”œâ”€â”€ table_01.py
â”‚           â”‚   â””â”€â”€ table_02.py
â”‚           â””â”€â”€ test/
â”‚               â”œâ”€â”€ test_business_rules.py
â”‚               â””â”€â”€ test_quality.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ env_utils.py
â”‚   â”‚   â”œâ”€â”€ init_env.py
â”‚   â”‚   â””â”€â”€ init_tables.py
â”‚   â””â”€â”€ dominio_1/
â”‚       â””â”€â”€ ingestion_utils.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš¡ï¸ DescripciÃ³n General

- **notebooks/**: Contiene notebooks organizados por dominio y capa del lakehouse (bronze, silver, gold, arquetipos).
- **src/**: CÃ³digo Python modularizado por dominio, con un espacio `common/` para utilidades globales y un submÃ³dulo por cada dominio de negocio.
- **requirements.txt**: Dependencias Python del proyecto.
- **.azure-pipelines/**: Definiciones de CI/CD para Azure DevOps.
- **README.md**: Este documento.
- **.gitignore**: Ignora archivos temporales y sensibles.

---

## ğŸ“ **Principios y Arquitectura**

- **Modularidad:** Cada dominio de negocio tiene sus notebooks, scripts y definiciÃ³n de tablas centralizados.
- **Gobernanza de datos:** Los schemas de cada tabla se definen en Python, con soporte a tipos, PK, particiones, comentarios y nullability.
- **SQL-first:** Las tablas Delta se crean vÃ­a `CREATE TABLE ...` usando el esquema y metadatos centralizados, asegurando PK, nulls y particiones desde el catÃ¡logo.
- **OrquestaciÃ³n automatizada:** Los notebooks pueden iterar sobre todos los esquemas definidos y crear tablas, comentarios, constraints y particiones de forma robusta.
- **Testing:** Pruebas de calidad y reglas de negocio por dominio.

---

## âš™ï¸ Requisitos Previos

- Azure Databricks (workspace configurado)
- Python 3.9+
- Azure DevOps (para CI/CD, opcional)
- Las dependencias listadas en `requirements.txt`

---

## ğŸ”§ **ConfiguraciÃ³n y Primeros Pasos**

1. **Clona el repositorio:**
   ```bash
   git clone https://github.com/tuusuario/databricks-estructura.git
   ```

2. **Configura tus variables de entorno**  
   Edita `src/common/init_env.py` segÃºn tu ambiente o usa variables de entorno Databricks para la conexiÃ³n.

3. **Define/ajusta tus tablas:**  
   En `notebooks/ingestion/dominio_x/tables/table_XX.py` define cada tabla, ejemplo:

   ```python
   def get_table_info():
       return {
           "schema": [
               {"name": "id", "type": "integer", "nullable": False, "comment": "ID Ãºnico"},
               {"name": "fecha", "type": "date", "nullable": False},
               {"name": "importe", "type": "decimal(18,4)", "nullable": False, "comment": "Importe de la venta"}
           ],
           "primary_key": ["id"],
           "partition_by": ["fecha"],
           "description": "Tabla bronze de ventas por fecha."
       }
   ```

4. **Ejecuta el notebook de creaciÃ³n de tablas:**  
   Abre y ejecuta `notebooks/ingestion/dominio_x/bronze/ntb_create_tables.ipynb`.  
   Este notebook, usando helpers como `schema_to_sql` y `create_table`, crea y documenta todas las tablas a partir de los esquemas modulares.

---

## âœ… **Buenas PrÃ¡cticas de ImportaciÃ³n (Databricks)**

- **Agrega `src/` al `sys.path` de tu notebook**  
  Esto asegura que puedes importar tus mÃ³dulos Python personalizados desde cualquier notebook.

- **Usa el bootstrap para inicializar rutas y contexto**  
  Al inicio de cada notebook, ejecuta:

  ```python
  %run ../../../bootstrap
  ```

  Esto aÃ±ade automÃ¡ticamente el directorio `src/` al path de Python y prepara el entorno.

- **Importa solo lo necesario segÃºn el dominio**
  - AsÃ­ mantienes tu cÃ³digo claro y modular.
  - Ejemplo recomendado en tu notebook:
    ```python
    from common.env_utils import get_env
    from dominio_1.ingestion_utils import funcion_critica
    ```

- **Evita imports globales o comodines (no uses `from src.* import *`)**
  - Solo importa las funciones, clases o utilidades que realmente necesitas para el notebook/domino en cuestiÃ³n.

---

## ğŸ“ **Testing de calidad y reglas de negocio**

- Coloca tus notebooks y scripts de pruebas en `notebooks/ingestion/dominio_x/test/`
- Recomendado: usar [Great Expectations](https://greatexpectations.io/) o pruebas personalizadas en PySpark.

---

## ğŸ” CI/CD

- El pipeline de Azure DevOps en `.azure-pipelines/pipeline.yml` permite automatizar pruebas, despliegues o validaciones del cÃ³digo.
- Puedes adaptar el pipeline para empaquetar el mÃ³dulo `src/` como library (`.whl`) para su uso productivo.

---

