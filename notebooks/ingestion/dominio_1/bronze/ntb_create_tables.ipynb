{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a39a34-96c3-4b6b-9e87-f365a66e6258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03d768f-ee2d-4421-8d88-8cf18b195610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib.util\n",
    "from src.common.init_tables import to_struct_type\n",
    "\n",
    "# Set the configuration to preserve char/varchar type information\n",
    "spark.conf.set(\"spark.sql.preserveCharVarcharTypeInfo\", \"true\")\n",
    "\n",
    "tables_path = os.path.abspath(os.path.join(os.getcwd(), \"../tables\"))\n",
    "tables_infos = []\n",
    "\n",
    "for fname in os.listdir(tables_path):\n",
    "    if fname.endswith(\".py\") and not fname.startswith(\"__\"):\n",
    "        fpath = os.path.join(tables_path, fname)\n",
    "        module_name = fname[:-3]\n",
    "        spec = importlib.util.spec_from_file_location(module_name, fpath)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        if hasattr(module, \"get_table_info\"):\n",
    "            info = module.get_table_info()\n",
    "            tables_infos.append(info)\n",
    "            print(f\"Procesando tabla: {info['table']}\")\n",
    "\n",
    "    # Importa fuera del ciclo, así solo se crea una vez por tabla\n",
    "    for info in tables_infos:\n",
    "        schema = to_struct_type(info[\"schema\"])\n",
    "\n",
    "        # 1. Crea DataFrame vacío con el schema declarado\n",
    "        df = spark.createDataFrame([], schema)\n",
    "\n",
    "        # 2. Escribe el Delta vacío (esto \"crea\" la estructura real del Delta)\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(info[\"path\"])\n",
    "\n",
    "        # 3. Registra la tabla Delta en el metastore\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {info['table']}\n",
    "            USING DELTA\n",
    "            LOCATION '{info['path']}'\n",
    "        \"\"\")\n",
    "\n",
    "        for col in info[\"schema\"]:\n",
    "            if \"comment\" in col and col[\"comment\"]:\n",
    "                spark.sql(f\"\"\"\n",
    "                    ALTER TABLE {info['table']} ALTER COLUMN {col['name']} COMMENT '{col['comment']}'\n",
    "                \"\"\")\n",
    "        print(f\"Tabla {info['table']} creada y registrada.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ntb_create_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
